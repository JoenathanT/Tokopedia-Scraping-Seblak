{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhIDkp7qX2R"
      },
      "source": [
        "## Tools Preparation\n",
        "Scrapping is basically one of the way to retrieve the data and this process is very important to know as a data scientist since sometimes we cannot get data easily as we querying the data from the database or download Kaggle. We're going to scrape Gramedia.com in this lesson using Beautifulsoup. Before we're going further, please install beautifulsoup.\n",
        "\n",
        "To install beautifulsoup, you may run one of the following commands on Anaconda Prompt (Windows) or Terminal (Linux/Mac/VSCode):\n",
        "\n",
        "```\n",
        "pip install bs4 selenium\n",
        "```\n",
        "\n",
        "and also you need to install requests to acces a web address by running:\n",
        "\n",
        "```\n",
        "pip install requests\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: selenium in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.19.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
            "Requirement already satisfied: trio~=0.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "pip install bs4 selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
            "Requirement already satisfied: selenium in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.19.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
            "Requirement already satisfied: trio~=0.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install bs4 selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: unknown command \"update\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip update request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement request (from versions: none)\n",
            "ERROR: No matching distribution found for request\n"
          ]
        }
      ],
      "source": [
        "pip install requests --upgrade request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxb42wTJIzyj"
      },
      "source": [
        "### Selenium WebDriver\n",
        "\n",
        "Selenium WebDriver is a powerful tool for automating browser interactions and testing web applications. It provides a programming interface to control browser behavior and perform actions such as clicking buttons, filling forms, and navigating through web pages.\n",
        "\n",
        "To get started with Selenium WebDriver for different browsers, you'll need to ensure that you have the appropriate browser drivers installed and set up correctly. Each browser requires its specific driver to communicate with Selenium.\n",
        "\n",
        "1. Google Chrome:\n",
        "   - You need to download the ChromeDriver executable and place it in a location that is in your system's PATH.\n",
        "   - Official ChromeDriver download page: https://sites.google.com/chromium.org/driver/\n",
        "\n",
        "2. Safari:\n",
        "   - SafariDriver is automatically installed with Safari on macOS.\n",
        "   - To enable it, go to Safari preferences, then to the 'Advanced' tab, and check the \"Show Develop menu in menu bar\" option.\n",
        "   - After that, in the Develop menu, go to \"Allow Remote Automation\" to enable SafariDriver.\n",
        "\n",
        "3. Firefox:\n",
        "   - You need to download the geckodriver executable and place it in a location that is in your system's PATH.\n",
        "   - Official geckodriver download page: https://github.com/mozilla/geckodriver/releases\n",
        "\n",
        "4. Microsoft Edge:\n",
        "   - For Microsoft Edge (Chromium-based version), you need to download the Microsoft Edge Driver (also known as MSEdgeDriver) and place it in a location that is in your system's PATH.\n",
        "   - Official MSEdgeDriver download page: https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
        "\n",
        "Once you have set up the appropriate drivers, you can use Selenium WebDriver in your preferred programming language (Python, Java, C#, etc.) to automate interactions with the browsers.\n",
        "\n",
        "Here's the codes of how to define Selenium WebDriver with Python for Chrome, Safari, Firefox, and Microsoft Edge:\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "# Create a new instance of the Chrome browser\n",
        "driver = webdriver.Chrome(\"/path/to/chromedriver\")\n",
        "\n",
        "# Create a new instance of the Safari browser\n",
        "driver = webdriver.Safari()\n",
        "\n",
        "# Create a new instance of the Firefox browser\n",
        "driver = webdriver.Firefox(\"/path/to/geckodriver\")\n",
        "\n",
        "# Create a new instance of the Microsoft Edge browser\n",
        "driver = webdriver.Edge(\"/path/to/msedgedriver\")\n",
        "```\n",
        "\n",
        "Similarly, you can use WebDriver with other browsers by using the appropriate driver for each browser and modifying the setup accordingly.\n",
        "\n",
        "Always ensure you are using the latest versions of Selenium WebDriver and browser drivers to avoid compatibility issues. You can check the Selenium official website (https://www.selenium.dev/) and the respective browser driver download pages for updates and documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aMoG9gqX2S"
      },
      "source": [
        "## Basic Web Component\n",
        "\n",
        "The website that you are scraping in this lesson contains several components. Those are:\n",
        "- HTML — the main content of the page.\n",
        "- CSS — used to add styling to make the page look nicer.\n",
        "- JS — Javascript files add interactivity to web pages.\n",
        "- Images — image formats, such as JPG and PNG, allow web pages to show pictures.\n",
        "\n",
        "There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look primarily at the HTML. Hence, you need to know some HTML structure to ease your scraping works. But don't worry, you don't need to dive in deeply into it.\n",
        "\n",
        "### HTML Structure\n",
        "\n",
        "HyperText Markup Language (HTML) is the language that web pages are created in. HTML isn’t a programming language, like Python, though. It’s a markup language that tells a browser how to display content.\n",
        "\n",
        "HTML has many functions that are similar to what you might find in a word processor like Microsoft Word — it can make text bold, create paragraphs, and so on.\n",
        "\n",
        "Below an example of HTML structure:\n",
        "\n",
        "```html\n",
        "<HTML>\n",
        "    <HEAD>\n",
        "        <TITLE>My cool title</TITLE>\n",
        "    </HEAD>\n",
        "    <BODY>\n",
        "        <H1>This is a Header</H1>\n",
        "        <ul id=\"list\" class=\"coolList\">\n",
        "            <li>item 1</li>\n",
        "            <li>item 2</li>\n",
        "            <li>item 3</li>\n",
        "        </ul>\n",
        "    </BODY>\n",
        "</HTML>\n",
        "```\n",
        "\n",
        "- The red items are called as tag or element. Usually, tag follows \"<\".\n",
        "- HTML, HEAD, and BODY are the main elements and the rests are the content. For your attention, we will focus on the contents.\n",
        "- The orange items are attribut that give information about the tag.\n",
        "- The blue texts are the attribute value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR5EdxCbqX2T"
      },
      "source": [
        "## Accessing the Web\n",
        "\n",
        "Now, we will access https://www.gramedia.com/categories/buku for this lesson. Before we go further, we need to understand how to access the url in Python. To do it, we use requests library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import selenium\n",
        "import bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UTxLTOM2qX2T",
        "outputId": "9ea29440-aa44-48c8-d343-64288edf5bb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "page = requests.get(\"https://www.gramedia.com/categories/buku\")\n",
        "page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koWR2L24qX2U"
      },
      "source": [
        "If you see the output is <Response [200]>, then you are success to access the url. \"200\" refers to HTTP status codes. You can read https://id.wikipedia.org/wiki/Daftar_kode_status_HTTP for further explaination.\n",
        "\n",
        "Now, you can check the HTML content of the page in Python. However, you can also check it on your browser by right click and choose Inspect element to ease your understanding od the web structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UepbpnGqX2U"
      },
      "source": [
        "Above is the HTML structure that Python successfully access. We need to parsing the structure using Beautifulsoup to make it clear and accessible to scrape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from webdriver-manager) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (2024.2.2)\n",
            "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.19.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
            "Requirement already satisfied: trio~=0.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BeautifulSoup' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://newafe.skkmigas.go.id/home/afe-coverpage?afeRevisionId=95328a80-8276-40c3-b2dc-ff4d5baeac6f\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m---> 12\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(soup\u001b[38;5;241m.\u001b[39mprettify()[:\u001b[38;5;241m700\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
          ]
        }
      ],
      "source": [
        "from selenium.webdriver import Chrome, ChromeOptions\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "\n",
        "options = ChromeOptions()\n",
        "options.add_argument(\"--start-maximized\")\n",
        "\n",
        "driver = Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "driver.get(\"https://newafe.skkmigas.go.id/home/afe-coverpage?afeRevisionId=95328a80-8276-40c3-b2dc-ff4d5baeac6f\", auth = 'magang.pokja', 'M4g4n92024*')\n",
        "\n",
        "html = driver.page_source\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "print(soup.prettify()[:700])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtNvku54qX2V",
        "outputId": "ac5db257-05ec-481b-ca6d-7946f98b6aa2"
      },
      "outputs": [
        {
          "ename": "NoSuchDriverException",
          "evalue": "Message: Unable to obtain driver for chrome using Selenium Manager.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[39m=\u001b[39m SeleniumManager()\u001b[39m.\u001b[39;49mdriver_location(options) \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:98\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     96\u001b[0m     args\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m---> 98\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(args)\n\u001b[0;32m    100\u001b[0m browser_path \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mbrowser_path\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:144\u001b[0m, in \u001b[0;36mSeleniumManager.run\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m completed_proc\u001b[39m.\u001b[39mreturncode:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mraise\u001b[39;00m WebDriverException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsuccessful command executed: \u001b[39m\u001b[39m{\u001b[39;00mcommand\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mstderr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;31mWebDriverException\u001b[0m: Message: Unsuccessful command executed: c:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\windows\\selenium-manager.exe --browser chrome --output json.\n{'code': 69, 'message': 'Driver unavailable: Driver path: ', 'driver_path': '', 'browser_path': ''}\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Jonathan T\\OneDrive\\文档\\bootcamp\\phase 0\\Web_scrapping\\P0W3D1PM_Web_Scraping.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m \u001b[39mimport\u001b[39;00m webdriver\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# chrome_driver_path = r'C:\\Users\\Jonathan T\\OneDrive\\文档\\bootcamp\\phase 0\\Web_scrapping\\chromedriver.exe'\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.gramedia.com/categories/buku\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m driver\u001b[39m.\u001b[39mget(url)\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[39m=\u001b[39m service \u001b[39mif\u001b[39;00m service \u001b[39melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mif\u001b[39;00m options \u001b[39melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     46\u001b[0m     DesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     47\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     48\u001b[0m     options,\n\u001b[0;32m     49\u001b[0m     service,\n\u001b[0;32m     50\u001b[0m     keep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:51\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvendor_prefix \u001b[39m=\u001b[39m vendor_prefix\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m service\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mpath \u001b[39m=\u001b[39m DriverFinder\u001b[39m.\u001b[39;49mget_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice, options)\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:41\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     40\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to obtain driver for \u001b[39m\u001b[39m{\u001b[39;00moptions\u001b[39m.\u001b[39mcapabilities[\u001b[39m'\u001b[39m\u001b[39mbrowserName\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m using Selenium Manager.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m Path(path)\u001b[39m.\u001b[39mis_file():\n\u001b[0;32m     44\u001b[0m     \u001b[39mraise\u001b[39;00m NoSuchDriverException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate or obtain driver for \u001b[39m\u001b[39m{\u001b[39;00moptions\u001b[39m.\u001b[39mcapabilities[\u001b[39m'\u001b[39m\u001b[39mbrowserName\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome using Selenium Manager.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "# chrome_driver_path = r'C:\\Users\\Jonathan T\\OneDrive\\文档\\bootcamp\\phase 0\\Web_scrapping\\chromedriver.exe'\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "url=\"https://www.gramedia.com/categories/buku\" \n",
        "driver.get(url)\n",
        "html = driver.page_source\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "print(soup.prettify()[:700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Tc-m7CqX2V"
      },
      "source": [
        "<img src=\"https://i.ibb.co/vsz2M33/message-Image-1636690176458.jpg\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfCNE9mxqX2V"
      },
      "source": [
        "Let that we want to retrieve the books' title, so let's check the title position on the HTML out using Inspect element!\n",
        "\n",
        "We know that based on the Inspect element, the books' title lie on this code:\n",
        "\n",
        "```html\n",
        "<div _ngcontent-web-gramedia-c53=\"\" class=\"list-title\">Creepy Case Club 4: Kasus Pohon Pemanggil</div>\n",
        "```\n",
        "\n",
        "\"Creepy Case Club 4: Kasus Pohon Pemanggil\" located at **div** tag with attribute **class** and the value of \"*list-title*\". So we will use the information to inform the soup where the titles exist.\n",
        "\n",
        "So we need to find all div elements that contain attribute class and value \"list-title\".\n",
        "\n",
        "To do that, we use ```soup.find_all(\"<element>\",{\"<attribute>\":\"<attribute value>\"})```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocCpMIvxqX2V",
        "outputId": "9f17a1e6-d252-4b12-8207-572c87ec167d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "soup.find_all('div',{\"class\":\"list-title\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pR8z7q5qX2W"
      },
      "source": [
        "We see that the soup found all div elements that contain attribute class and value \"list-title\" but we need the title text only. To extract it, just add .get_text() method to each list element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KLV9ZPFqX2W",
        "outputId": "2fc0515d-c9ad-4c89-e388-82ae3d21d0fe"
      },
      "outputs": [],
      "source": [
        "for div_tag in soup.find_all('div',{\"class\":\"list-title\"}):\n",
        "    print(div_tag.get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aZ0uO09qX2W"
      },
      "source": [
        "It is easy, isn't it?\n",
        "\n",
        "Next, we will do more. Our task is to get information about Title, Author, Price, Link to the book's page, and link refers to image.\n",
        "\n",
        "Based on the Inspect element, we know that those information locate on:\n",
        "- Title: ```<div _ngcontent-web-gramedia-c53=\"\" class=\"list-title\">Creepy Case Club 4: Kasus Pohon Pemanggil</div>```\n",
        "- Author: ```<p class=\"div-author\"><span _ngcontent-web-gramedia-c53=\"\" class=\"list-author ng-star-inserted\"> Arvidan None </span>```\n",
        "- Price: ```<p _ngcontent-web-gramedia-c53=\"\" class=\"formats-price\">Rp 79.000</p>```\n",
        "- Link: ```<div class=\"ng-star-inserted\"><a _ngcontent-web-gramedia-c53=\"\" href=\"/products/think-and-grow-rich-cara-para-jutawan-dan-miliarder-meraih-kekayaan\">```\n",
        "- Image: ```<img _ngcontent-web-gramedia-c26=\"\" class=\"product-list-img ng-star-inserted ng-lazyloaded\" src=\"https://cdn.gramedia.com/uploads/items/9786230405990_Think_and_Grow_Rich__w149_hauto.jpeg\" alt=\"Think And Grow Rich : Cara Para Jutawan Dan Miliarder Meraih Kekayaan\">```\n",
        "\n",
        "Let's we wrap up the code and then input the data into Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgSH13v0qX2W",
        "outputId": "5ee9900e-9fb2-45d0-cebb-4fdd04d20694"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Price</th>\n",
              "      <th>Image</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Teasing Master, Takagi 5</td>\n",
              "      <td>YAMAMOTO SOUICHIROU</td>\n",
              "      <td>Rp 30.000</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/7220101...</td>\n",
              "      <td>https://www.gramedia.com/products/teasing-mast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Teasing Master, Takagi 6.</td>\n",
              "      <td>YAMAMOTO SOUICHIROU</td>\n",
              "      <td>Rp 30.000</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/7220102...</td>\n",
              "      <td>https://www.gramedia.com/products/teasing-mast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Teasing Master, Takagi 7</td>\n",
              "      <td>YAMAMOTO SOUICHIROU</td>\n",
              "      <td>Rp 30.000</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/7220103...</td>\n",
              "      <td>https://www.gramedia.com/products/teasing-mast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lima Sekawan: Karang Setan</td>\n",
              "      <td>Enid Blyton</td>\n",
              "      <td>Rp 32.250</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/9786020...</td>\n",
              "      <td>https://www.gramedia.com/products/lima-sekawan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lima Sekawan: Rahasia di Pulau Kirrin</td>\n",
              "      <td>Enid Blyton</td>\n",
              "      <td>Rp 32.250</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/9786020...</td>\n",
              "      <td>https://www.gramedia.com/products/lima-sekawan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hai Miiko Vol 34</td>\n",
              "      <td>Ono Eriko</td>\n",
              "      <td>Rp 27.200</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/HAI_MII...</td>\n",
              "      <td>https://www.gramedia.com/products/hai-miiko-34...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>And Then There Were None (Lalu Semuanya Lenyap)</td>\n",
              "      <td></td>\n",
              "      <td>Rp 44.000</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/9789792...</td>\n",
              "      <td>https://www.gramedia.com/products/lalu-semuany...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Horimiya 16</td>\n",
              "      <td>Hero, Daisuke Hagiwara</td>\n",
              "      <td>Rp 24.000</td>\n",
              "      <td>https://cdn.gramedia.com/uploads/items/7220101...</td>\n",
              "      <td>https://www.gramedia.com/products/horimiya-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Akasha: Futo Detective 3</td>\n",
              "      <td>Sanjo Riku/ Masaki Sato</td>\n",
              "      <td>Rp 33.750</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/akasha-futo-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TTS Otak - Atik Angka</td>\n",
              "      <td>Yulius Yuwana</td>\n",
              "      <td>Rp 55.250</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/tts-otak-ati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Cerpen Pilihan Kompas 2020: Macan</td>\n",
              "      <td>Kumpulan Cerpen</td>\n",
              "      <td>Rp 65.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/cerpen-pilih...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Tts Anak Kompas - Jilid 1</td>\n",
              "      <td>Soepono As</td>\n",
              "      <td>Rp 65.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/tts-anak-kom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Fairy Tail 100 years Quest 08</td>\n",
              "      <td>Hiro Mashima</td>\n",
              "      <td>Rp 30.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/fairy-tail-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Tanya Jawab Seru Tentang Lautan</td>\n",
              "      <td>Miles Kelly</td>\n",
              "      <td>Rp 41.250</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/tanya-jawab-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Level Comic: Attack on Titan 34</td>\n",
              "      <td></td>\n",
              "      <td>Rp 30.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/lc-attack-on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Mata Hari (The Spy)</td>\n",
              "      <td>Paulo Coelho</td>\n",
              "      <td>Rp 37.500</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/conf-mata-ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Ayo, Mewarnai Objek Wisata Indonesia</td>\n",
              "      <td>Tim Bip</td>\n",
              "      <td>Rp 24.375</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/buku-ayo-mew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PR Interaktif Pendidikan Jasmani, Olahraga, da...</td>\n",
              "      <td>Faqih Choeroni</td>\n",
              "      <td>Rp 22.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/pr-interakti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>N Or M</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>Rp 37.500</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/n-or-m-cover...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Akasha: Oshi no Ko - Anak Idola 3</td>\n",
              "      <td></td>\n",
              "      <td>Rp 36.000</td>\n",
              "      <td>/assets/default-images/product.png</td>\n",
              "      <td>https://www.gramedia.com/products/akasha-oshi-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Title  \\\n",
              "0                            Teasing Master, Takagi 5   \n",
              "1                           Teasing Master, Takagi 6.   \n",
              "2                            Teasing Master, Takagi 7   \n",
              "3                          Lima Sekawan: Karang Setan   \n",
              "4               Lima Sekawan: Rahasia di Pulau Kirrin   \n",
              "5                                    Hai Miiko Vol 34   \n",
              "6     And Then There Were None (Lalu Semuanya Lenyap)   \n",
              "7                                         Horimiya 16   \n",
              "8                            Akasha: Futo Detective 3   \n",
              "9                               TTS Otak - Atik Angka   \n",
              "10                  Cerpen Pilihan Kompas 2020: Macan   \n",
              "11                          Tts Anak Kompas - Jilid 1   \n",
              "12                      Fairy Tail 100 years Quest 08   \n",
              "13                    Tanya Jawab Seru Tentang Lautan   \n",
              "14                    Level Comic: Attack on Titan 34   \n",
              "15                                Mata Hari (The Spy)   \n",
              "16               Ayo, Mewarnai Objek Wisata Indonesia   \n",
              "17  PR Interaktif Pendidikan Jasmani, Olahraga, da...   \n",
              "18                                             N Or M   \n",
              "19                  Akasha: Oshi no Ko - Anak Idola 3   \n",
              "\n",
              "                       Author      Price  \\\n",
              "0        YAMAMOTO SOUICHIROU   Rp 30.000   \n",
              "1        YAMAMOTO SOUICHIROU   Rp 30.000   \n",
              "2        YAMAMOTO SOUICHIROU   Rp 30.000   \n",
              "3                Enid Blyton   Rp 32.250   \n",
              "4                Enid Blyton   Rp 32.250   \n",
              "5                  Ono Eriko   Rp 27.200   \n",
              "6                              Rp 44.000   \n",
              "7     Hero, Daisuke Hagiwara   Rp 24.000   \n",
              "8    Sanjo Riku/ Masaki Sato   Rp 33.750   \n",
              "9              Yulius Yuwana   Rp 55.250   \n",
              "10           Kumpulan Cerpen   Rp 65.000   \n",
              "11                Soepono As   Rp 65.000   \n",
              "12              Hiro Mashima   Rp 30.000   \n",
              "13               Miles Kelly   Rp 41.250   \n",
              "14                             Rp 30.000   \n",
              "15              Paulo Coelho   Rp 37.500   \n",
              "16                   Tim Bip   Rp 24.375   \n",
              "17            Faqih Choeroni   Rp 22.000   \n",
              "18           Agatha Christie   Rp 37.500   \n",
              "19                             Rp 36.000   \n",
              "\n",
              "                                                Image  \\\n",
              "0   https://cdn.gramedia.com/uploads/items/7220101...   \n",
              "1   https://cdn.gramedia.com/uploads/items/7220102...   \n",
              "2   https://cdn.gramedia.com/uploads/items/7220103...   \n",
              "3   https://cdn.gramedia.com/uploads/items/9786020...   \n",
              "4   https://cdn.gramedia.com/uploads/items/9786020...   \n",
              "5   https://cdn.gramedia.com/uploads/items/HAI_MII...   \n",
              "6   https://cdn.gramedia.com/uploads/items/9789792...   \n",
              "7   https://cdn.gramedia.com/uploads/items/7220101...   \n",
              "8                  /assets/default-images/product.png   \n",
              "9                  /assets/default-images/product.png   \n",
              "10                 /assets/default-images/product.png   \n",
              "11                 /assets/default-images/product.png   \n",
              "12                 /assets/default-images/product.png   \n",
              "13                 /assets/default-images/product.png   \n",
              "14                 /assets/default-images/product.png   \n",
              "15                 /assets/default-images/product.png   \n",
              "16                 /assets/default-images/product.png   \n",
              "17                 /assets/default-images/product.png   \n",
              "18                 /assets/default-images/product.png   \n",
              "19                 /assets/default-images/product.png   \n",
              "\n",
              "                                                 Link  \n",
              "0   https://www.gramedia.com/products/teasing-mast...  \n",
              "1   https://www.gramedia.com/products/teasing-mast...  \n",
              "2   https://www.gramedia.com/products/teasing-mast...  \n",
              "3   https://www.gramedia.com/products/lima-sekawan...  \n",
              "4   https://www.gramedia.com/products/lima-sekawan...  \n",
              "5   https://www.gramedia.com/products/hai-miiko-34...  \n",
              "6   https://www.gramedia.com/products/lalu-semuany...  \n",
              "7       https://www.gramedia.com/products/horimiya-16  \n",
              "8   https://www.gramedia.com/products/akasha-futo-...  \n",
              "9   https://www.gramedia.com/products/tts-otak-ati...  \n",
              "10  https://www.gramedia.com/products/cerpen-pilih...  \n",
              "11  https://www.gramedia.com/products/tts-anak-kom...  \n",
              "12  https://www.gramedia.com/products/fairy-tail-1...  \n",
              "13  https://www.gramedia.com/products/tanya-jawab-...  \n",
              "14  https://www.gramedia.com/products/lc-attack-on...  \n",
              "15  https://www.gramedia.com/products/conf-mata-ha...  \n",
              "16  https://www.gramedia.com/products/buku-ayo-mew...  \n",
              "17  https://www.gramedia.com/products/pr-interakti...  \n",
              "18  https://www.gramedia.com/products/n-or-m-cover...  \n",
              "19  https://www.gramedia.com/products/akasha-oshi-...  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "data['Title'] = [ title.get_text() for title in soup.find_all( 'div', {\"class\":\"list-title\"} ) ]\n",
        "data['Author'] = [ author.get_text() for author in soup.find_all( 'p', {\"class\":\"div-author\"} ) ]\n",
        "data['Price'] = [ price.get_text() for price in soup.find_all( 'p', {\"class\":\"formats-price\"} ) ]\n",
        "data['Image'] = [ img['src'] for img in soup.find_all( 'img',{\"class\":\"product-list-img\"} ) ]\n",
        "\n",
        "links = []\n",
        "for tag in soup.find_all( 'div',{\"_ngcontent-web-gramedia-c26\":\"\",\"class\":\"ng-star-inserted\"} ):\n",
        "    try:\n",
        "        links.append(\"https://www.gramedia.com\"+tag.find_all('a',{\"_ngcontent-web-gramedia-c26\":\"\"})[0]['href'])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "data['Link'] = links\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsdKkZT7qX2X"
      },
      "source": [
        "## Multipage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxMlkvAqX2X"
      },
      "source": [
        "Currently, we are working on a page. However, the rest of the web consist of more pages like below:\n",
        "\n",
        "<img src=\"https://i.ibb.co/CQ6JQLv/message-Image-1636716930335.jpg\"></img>\n",
        "\n",
        "If we look at the next page such as page 2, we can see that the url change to https://www.gramedia.com/categories/buku?page=2 and page 3: https://www.gramedia.com/categories/buku?page=3. Then we know each page has a numbering format on url so we can access many pages one time automatically using loop. We exclude the image since image loader is very depended on your connection. Let's check the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0q1NjS2qX2X",
        "outputId": "529ee2d2-8740-4c5a-d89f-476892c6b7d1"
      },
      "outputs": [
        {
          "ename": "NoSuchWindowException",
          "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.150)\nStacktrace:\n\tGetHandleVerifier [0x007ACFE3+45267]\n\t(No symbol) [0x00739741]\n\t(No symbol) [0x0062BE1D]\n\t(No symbol) [0x006134AE]\n\t(No symbol) [0x0067EFDB]\n\t(No symbol) [0x0068D913]\n\t(No symbol) [0x0067AE36]\n\t(No symbol) [0x0065674E]\n\t(No symbol) [0x006578ED]\n\tGetHandleVerifier [0x00A65659+2897737]\n\tGetHandleVerifier [0x00AAE78B+3197051]\n\tGetHandleVerifier [0x00AA8571+3171937]\n\tGetHandleVerifier [0x00835E40+606000]\n\t(No symbol) [0x0074338C]\n\t(No symbol) [0x0073F508]\n\t(No symbol) [0x0073F62F]\n\t(No symbol) [0x00731D27]\n\tBaseThreadInitThunk [0x75937BA9+25]\n\tRtlInitializeExceptionChain [0x7744B79B+107]\n\tRtlClearBits [0x7744B71F+191]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Jonathan T\\OneDrive\\文档\\bootcamp\\phase 0\\Web_scrapping\\P0W3D1PM_Web_Scraping.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.gramedia.com/categories/buku?page=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m driver\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m html \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mpage_source\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m title \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [ title\u001b[39m.\u001b[39mget_text() \u001b[39mfor\u001b[39;00m title \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all( \u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mlist-title\u001b[39m\u001b[39m\"\u001b[39m} ) ]\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpage_source\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[0;32m    440\u001b[0m \u001b[39m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39m            driver.page_source\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET_PAGE_SOURCE)[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
            "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.150)\nStacktrace:\n\tGetHandleVerifier [0x007ACFE3+45267]\n\t(No symbol) [0x00739741]\n\t(No symbol) [0x0062BE1D]\n\t(No symbol) [0x006134AE]\n\t(No symbol) [0x0067EFDB]\n\t(No symbol) [0x0068D913]\n\t(No symbol) [0x0067AE36]\n\t(No symbol) [0x0065674E]\n\t(No symbol) [0x006578ED]\n\tGetHandleVerifier [0x00A65659+2897737]\n\tGetHandleVerifier [0x00AAE78B+3197051]\n\tGetHandleVerifier [0x00AA8571+3171937]\n\tGetHandleVerifier [0x00835E40+606000]\n\t(No symbol) [0x0074338C]\n\t(No symbol) [0x0073F508]\n\t(No symbol) [0x0073F62F]\n\t(No symbol) [0x00731D27]\n\tBaseThreadInitThunk [0x75937BA9+25]\n\tRtlInitializeExceptionChain [0x7744B79B+107]\n\tRtlClearBits [0x7744B71F+191]\n"
          ]
        }
      ],
      "source": [
        "title = []\n",
        "author = []\n",
        "price = []\n",
        "image = []\n",
        "Links = []\n",
        "\n",
        "# driver = webdriver.Chrome('/Users/fahmimn21/Downloads/chromedriver')\n",
        "driver = Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "for i in range(1,21):\n",
        "    url=\"https://www.gramedia.com/categories/buku?page={}\".format(i)\n",
        "    driver.get(url)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    title += [ title.get_text() for title in soup.find_all( 'div', {\"class\":\"list-title\"} ) ]\n",
        "    author += [ author.get_text() for author in soup.find_all( 'p', {\"class\":\"div-author\"} ) ]\n",
        "    price += [ price.get_text() for price in soup.find_all( 'p', {\"class\":\"formats-price\"} ) ]\n",
        "    image += [ img['src'] for img in soup.find_all( 'img',{\"class\":\"product-list-img\"} ) ]\n",
        "\n",
        "    links = []\n",
        "    for tag in soup.find_all( 'div',{\"_ngcontent-web-gramedia-c26\":\"\",\"class\":\"ng-star-inserted\"} ):\n",
        "        try:\n",
        "            links.append(\"https://www.gramedia.com\"+tag.find_all('a',{\"_ngcontent-web-gramedia-c26\":\"\"})[0]['href'])\n",
        "        except:\n",
        "            pass\n",
        "    Links += links\n",
        "\n",
        "data_multipage = pd.DataFrame()\n",
        "data_multipage['Title'] = title\n",
        "data_multipage['Author'] = author\n",
        "data_multipage['Price'] = price\n",
        "data_multipage['Image'] = image\n",
        "data_multipage['Link'] = Links\n",
        "\n",
        "data_multipage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCNbYWDqX2X"
      },
      "source": [
        "## Accessing Individual Page\n",
        "\n",
        "<img src=\"https://i.ibb.co/F8D5bCy/message-Image-1637134633305.jpg\"></img>\n",
        "\n",
        "Suppose that we want to get more detail information about the books, but the information are on the individual page. So, we will access the individual page and scrape some information on it. We will catch title, author, price, description, number of pages, date of issue and publisher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh32wnYgqX2Y",
        "outputId": "fdf5a0a5-09fb-428b-8314-b9df2bec845d"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'capabilities'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[39m=\u001b[39m SeleniumManager()\u001b[39m.\u001b[39;49mdriver_location(options) \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:79\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determines the path of the correct driver.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[39m:Args:\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m - browser: which browser to get the driver path for.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39m:Returns: The driver path to use\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m browser \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39;49mcapabilities[\u001b[39m\"\u001b[39m\u001b[39mbrowserName\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     81\u001b[0m args \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_binary()), \u001b[39m\"\u001b[39m\u001b[39m--browser\u001b[39m\u001b[39m\"\u001b[39m, browser]\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Jonathan T\\OneDrive\\文档\\bootcamp\\phase 0\\Web_scrapping\\P0W3D1PM_Web_Scraping.ipynb Cell 26\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m date_issue \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m publisher \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(\u001b[39m'\u001b[39;49m\u001b[39m/Users/fahmimn21/Downloads/chromedriver\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jonathan%20T/OneDrive/%E6%96%87%E6%A1%A3/bootcamp/phase%200/Web_scrapping/P0W3D1PM_Web_Scraping.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.gramedia.com/categories/buku?page=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i)\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[39m=\u001b[39m service \u001b[39mif\u001b[39;00m service \u001b[39melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mif\u001b[39;00m options \u001b[39melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     46\u001b[0m     DesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     47\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     48\u001b[0m     options,\n\u001b[0;32m     49\u001b[0m     service,\n\u001b[0;32m     50\u001b[0m     keep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:51\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvendor_prefix \u001b[39m=\u001b[39m vendor_prefix\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m service\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mpath \u001b[39m=\u001b[39m DriverFinder\u001b[39m.\u001b[39;49mget_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice, options)\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Jonathan T\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:40\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     38\u001b[0m     path \u001b[39m=\u001b[39m SeleniumManager()\u001b[39m.\u001b[39mdriver_location(options) \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m---> 40\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to obtain driver for \u001b[39m\u001b[39m{\u001b[39;00moptions\u001b[39m.\u001b[39;49mcapabilities[\u001b[39m'\u001b[39m\u001b[39mbrowserName\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m using Selenium Manager.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[39mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m Path(path)\u001b[39m.\u001b[39mis_file():\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'"
          ]
        }
      ],
      "source": [
        "title = []\n",
        "author = []\n",
        "price = []\n",
        "desc = []\n",
        "num_pages = []\n",
        "date_issue = []\n",
        "publisher = []\n",
        "\n",
        "driver = webdriver.Chrome('/Users/fahmimn21/Downloads/chromedriver')\n",
        "\n",
        "for i in range(1,3):\n",
        "    url=\"https://www.gramedia.com/categories/buku?page={}\".format(i)\n",
        "    driver.get(url)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    for tag in soup.find_all( 'div',{\"_ngcontent-web-gramedia-c26\":\"\",\"class\":\"ng-star-inserted\"} ):\n",
        "        try:\n",
        "            link = \"https://www.gramedia.com\"+tag.find('a',{\"_ngcontent-web-gramedia-c26\":\"\"})['href']\n",
        "            driver.get(link)\n",
        "            html_ind = driver.page_source\n",
        "            soup_ind = BeautifulSoup(html_ind, \"html.parser\")\n",
        "\n",
        "            title.append( soup_ind.find( 'div', {\"class\":\"book-title\"} ).get_text() )\n",
        "            author.append( soup_ind.find('span',{\"class\":\"title-author\"}).get_text() )\n",
        "            price.append( soup_ind.find('div', {'class':'price-product'}).get_text() )\n",
        "            desc.append( soup_ind.find('pre').get_text() )\n",
        "            num_pages.append( soup_ind.find('div',{'class':'detail-section'}).find_all('p')[0].get_text() )\n",
        "            date_issue.append( soup_ind.find('div',{'class':'detail-section'}).find_all('p')[2].get_text() )\n",
        "            publisher.append( soup_ind.find('div',{'class':'detail-section'}).find_all('p')[1].get_text() )\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "pages = pd.DataFrame()\n",
        "pages['Title'] = title\n",
        "pages['Author'] = author\n",
        "pages['Price'] = price\n",
        "pages['Desc'] = desc\n",
        "pages['Num Pages'] = num_pages\n",
        "pages['Date Issue'] = date_issue\n",
        "pages['Publisher'] = publisher\n",
        "\n",
        "pages"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
